{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Activity 1 & Activity 2",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6qZz02VPWw5kq6a/cvcij",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhruvin-bagadia/githib.test/blob/main/Activity_1_%26_Activity_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZzkJTTE_sjU",
        "outputId": "c2a1b90c-a99a-48c0-ea2f-b22f0da7f8cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install tensorflow==1.15.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 19kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.12.4)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 33.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.33.1)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 46.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.18.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.35.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.0) (50.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=2acda6cf363b90bcd489648295b6ebe7773a86a5a4c1dc4acde61e2bb0255e5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-applications, gast, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15UN2yB2_4Y-"
      },
      "source": [
        "import tqdm\n",
        "import os\n",
        "import tensorflow as tf\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcWNKkdj__up",
        "outputId": "4f7fe7d0-e2d4-40a6-ec48-5bfee3a695b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install os"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for os\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi4T_00JAOZR",
        "outputId": "bda9a12e-38aa-40bf-effe-c65722e2f33a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz0ObfjbE4CA"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9pdAlDOE4St"
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZJ1Bh4IE4ZV"
      },
      "source": [
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mNGHZuHAE34",
        "outputId": "e42f1ea8-5895-488c-c212-02736d1914d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\"\"\"Copyright 2017 Google, Inc. All Rights Reserved.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "----\n",
        "Original script can be found at\n",
        "\n",
        "    https://github.com/dandelionmane/tf-dev-summit-tensorboard-tutorial\n",
        "\n",
        "And is similar to the TensorFlow tutorial script found at\n",
        "\n",
        "    https://www.tensorflow.org/get_started/mnist/mechanics\n",
        "\"\"\"\n",
        "import tqdm\n",
        "import os\n",
        "import tensorflow as tf\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "#\n",
        "#  Script variables. Change these and observe\n",
        "#  how results change.\n",
        "#\n",
        "LEARNING_RATE = 0.0001\n",
        "EPOCHS = 2000\n",
        "\n",
        "#\n",
        "#  Script constants. Here we pull data available\n",
        "#  locally. The data contains both the sprites\n",
        "#  and the labels from the original MNIST dataset.\n",
        "#\n",
        "LOGDIR = \"/content/drive/My Drive/Dataset1\"\n",
        "classes = ['labels_1024.tsv','sprite_1024.png']\n",
        "LABELS = os.path.join(os.getcwd(), \"/content/drive/My Drive/labels_1024.tsv\")\n",
        "SPRITES = os.path.join(os.getcwd(),\"/content/drive/My Drive/sprite_1024.png\" )\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Load data form the MNIST dataset into memory.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    TensorFlow object with dataset.\n",
        "    \"\"\"\n",
        "    if not (os.path.isfile(LABELS) and os.path.isfile(SPRITES)):\n",
        "        raise ValueError(\"\"\"\n",
        "            Necessary data files were not found. Make sure the files\n",
        "\n",
        "                * labels_1024.tsv\n",
        "                * sprite_1024.png\n",
        "\n",
        "            are available in the same location where you run this script.\n",
        "            \"\"\")\n",
        "\n",
        "    return tf.contrib.learn.datasets.mnist.read_data_sets(\n",
        "        train_dir=LOGDIR + \"data\", one_hot=True)\n",
        "\n",
        "\n",
        "def convolutional_layer(input, size_in, size_out, name=\"convolutional\"):\n",
        "    \"\"\"Convoluted layer.\n",
        "\n",
        "    Create the weights and biases distributions.\n",
        "    Also define the convolution and the activation function (ReLU).\n",
        "    Finally, we create some histogram summaries useful for TensorBoard.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    size_in, size_out: int or float\n",
        "        Where to truncate the normal distribution.\n",
        "\n",
        "    name: str\n",
        "        Name to give the TensorFlow scope.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(name):\n",
        "\n",
        "        W = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.1), name=\"Weights\")\n",
        "        B = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"Biases\")\n",
        "\n",
        "        convolution = tf.nn.conv2d(input, W, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
        "        activation = tf.nn.relu(convolution + B)\n",
        "\n",
        "        tf.summary.histogram(\"weights\", W)\n",
        "        tf.summary.histogram(\"biases\", B)\n",
        "        tf.summary.histogram(\"activations\", activation)\n",
        "\n",
        "        return tf.nn.max_pool(activation, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
        "\n",
        "\n",
        "def fully_connected_layer(input, size_in, size_out, name=\"fully_connected\"):\n",
        "    \"\"\"Fully connected layer.\n",
        "\n",
        "    This defines the fully connected layer.\n",
        "    Different from the convolution layer, this layer does not\n",
        "    perform a convolution but only defines an activation function.\n",
        "    That function is also different from the convolution layer\n",
        "    by multipliying the input data with its weights plus the biases,\n",
        "    which is a much simpler activation function than ReLU.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    size_in, size_out: int or float\n",
        "        Where to truncate the normal distribution.\n",
        "\n",
        "    name: str\n",
        "        Name to give the TensorFlow scope.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(name):\n",
        "        W = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"Weights\")\n",
        "        B = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"Biases\")\n",
        "\n",
        "        activation = tf.matmul(input, W) + B\n",
        "\n",
        "        tf.summary.histogram(\"weights\", W)\n",
        "        tf.summary.histogram(\"biases\", B)\n",
        "        tf.summary.histogram(\"activations\", activation)\n",
        "\n",
        "        return activation\n",
        "\n",
        "\n",
        "def model(mnist, learning_rate, epochs=2000):\n",
        "    \"\"\"Neural network model used in the MNIST dataset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mnist: TensorFlow dataset object\n",
        "        MNIST dataset loaded using TensorFlow.\n",
        "\n",
        "    learning_rate: float\n",
        "        Learning rate at which the network should\n",
        "        create momentum.\n",
        "\n",
        "    epochs: int, default 2000\n",
        "        Number of epochs to train the model with.\n",
        "    \"\"\"\n",
        "    name = \"MNIST-model/lr={}-epochs={}\".format(learning_rate, epochs)\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    sess = tf.Session()\n",
        "\n",
        "    X = tf.placeholder(tf.float32, shape=[None, 784], name=\"X\")\n",
        "    X_image = tf.reshape(X, [-1, 28, 28, 1])\n",
        "    tf.summary.image('input', X_image, 3)\n",
        "\n",
        "    Y = tf.placeholder(tf.float32, shape=[None, 10], name=\"Labels\")\n",
        "\n",
        "    #\n",
        "    #  Convolutional layer treatment. We use a single convolutional layer.\n",
        "    #\n",
        "    convolution = convolutional_layer(X_image, 1, 64, \"Convolution_Layer\")\n",
        "    convolution_output = tf.nn.max_pool(convolution, ksize=[1, 2, 2, 1],\n",
        "                                        strides=[1, 2, 2, 1], padding=\"SAME\")\n",
        "\n",
        "    flattened = tf.reshape(convolution_output, [-1, 7 * 7 * 64])\n",
        "\n",
        "    #\n",
        "    #  Fully-connected layer treatment.\n",
        "    #  We will use two fully connected layers\n",
        "    #  that connect to each other. We can\n",
        "    #  try more or less to see the impact\n",
        "    #  on our model.\n",
        "    #\n",
        "    fully_connected_1 = fully_connected_layer(flattened, 7 * 7 * 64, 1024,\n",
        "                                              \"Fully-connected_Layer_1\")\n",
        "    relu = tf.nn.relu(fully_connected_1)\n",
        "    embedding_input = relu\n",
        "    tf.summary.histogram(\"Fully-connected_Layer-1/relu\", relu)\n",
        "\n",
        "    embedding_size = 1024\n",
        "    logits = fully_connected_layer(fully_connected_1, 1024, 10,\n",
        "                                   \"Fully-connected_Layer_2\")\n",
        "\n",
        "    with tf.name_scope(\"Cross_Entropy\"):\n",
        "        cross_entropy = tf.reduce_mean(\n",
        "            tf.nn.softmax_cross_entropy_with_logits(\n",
        "                logits=logits, labels=Y), name=\"cross_entropy\")\n",
        "\n",
        "        tf.summary.scalar(\"cross_entropy\", cross_entropy)\n",
        "\n",
        "    with tf.name_scope(\"Train\"):\n",
        "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
        "\n",
        "    with tf.name_scope(\"Accuracy\"):\n",
        "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "        tf.summary.scalar(\"Accuracy\", accuracy)\n",
        "\n",
        "    summ = tf.summary.merge_all()\n",
        "\n",
        "    #\n",
        "    #  Let's save embeddings so that they\n",
        "    #  are available in TensorBoard.\n",
        "    #\n",
        "    embedding = tf.Variable(tf.zeros([1024, embedding_size]), name=\"Test_Embedding\")\n",
        "    assignment = embedding.assign(embedding_input)\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    #\n",
        "    #  We can now run our TensorFlow session.\n",
        "    #\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    writer = tf.summary.FileWriter(LOGDIR + name)\n",
        "    writer.add_graph(sess.graph)\n",
        "\n",
        "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
        "    embedding_config = config.embeddings.add()\n",
        "    embedding_config.tensor_name = embedding.name\n",
        "    embedding_config.sprite.image_path = SPRITES\n",
        "    embedding_config.metadata_path = LABELS\n",
        "\n",
        "    #\n",
        "    #  Create each thumbnail.\n",
        "    #\n",
        "    embedding_config.sprite.single_image_dim.extend([28, 28])\n",
        "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(writer, config)\n",
        "\n",
        "    for i in tqdm.tqdm(range(epochs + 1), 'Epochs'):\n",
        "        batch = mnist.train.next_batch(100)\n",
        "\n",
        "        if i % 5 == 0:\n",
        "            [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={X: batch[0], Y: batch[1]})\n",
        "            writer.add_summary(s, i)\n",
        "        if i % 500 == 0:\n",
        "            sess.run(assignment, feed_dict={\n",
        "                X: mnist.test.images[:1024],\n",
        "                Y: mnist.test.labels[:1024]\n",
        "            })\n",
        "            saver.save(sess, os.path.join(LOGDIR, \"model.ckpt\"), i)\n",
        "\n",
        "        sess.run(train_step, feed_dict={X: batch[0], Y: batch[1]})\n",
        "\n",
        "    print('Done training!')\n",
        "    print('Run `tensorboard --logdir=%s` to see the results.' % LOGDIR)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main runner function.\n",
        "\n",
        "    This calls the model function.\n",
        "    \"\"\"\n",
        "    print(\"\"\"\n",
        "    Running MNIST model with:\n",
        "\n",
        "        * Learning rate: {}\n",
        "        * Epochs: {}\n",
        "\n",
        "    \"\"\".format(LEARNING_RATE, EPOCHS))\n",
        "\n",
        "    mnist = load_data()\n",
        "    model(mnist=mnist, learning_rate=LEARNING_RATE, epochs=EPOCHS)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "    Running MNIST model with:\n",
            "\n",
            "        * Learning rate: 0.0001\n",
            "        * Epochs: 2000\n",
            "\n",
            "    \n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting /content/drive/My Drive/Dataset1data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting /content/drive/My Drive/Dataset1data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /content/drive/My Drive/Dataset1data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /content/drive/My Drive/Dataset1data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 2001/2001 [07:03<00:00,  4.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done training!\n",
            "Run `tensorboard --logdir=/content/drive/My Drive/Dataset1` to see the results.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78czUylhVUae"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}